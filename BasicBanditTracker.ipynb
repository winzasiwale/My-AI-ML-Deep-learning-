{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90841f62-0a9c-43c3-99ce-cbd9162f1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Basic Record Keeping\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STEP 1: The Absolute Basics - Just Record Everything\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Basic Record Keeping\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class BasicBanditTracker:\n",
    "    \"\"\"\n",
    "    The simplest possible bandit - just keeps track of what happens.\n",
    "    No intelligence yet, just data collection.\n",
    "\n",
    "    This helps students see what information we're working with\n",
    "    before we worry about how to use it smartly.\n",
    "    \"\"\"\n",
    "    def __init__(self, k_arms):\n",
    "        self.k = k_arms\n",
    "\n",
    "        # Store every single action and reward we see\n",
    "        # This is inefficient but helps students understand the data\n",
    "        self.action_history = []    # Which arm was pulled each time\n",
    "        self.reward_history = []    # What reward we got each time\n",
    "\n",
    "        print(f\"Created a {k_arms}-armed bandit tracker\")\n",
    "        print(\"I'll remember every action and reward, but I won't make smart decisions yet\")\n",
    "\n",
    "    def record_action_and_reward(self, action, reward):\n",
    "        \"\"\"Just record what happened - no learning yet.\"\"\"\n",
    "        self.action_history.append(action)\n",
    "        self.reward_history.append(reward)\n",
    "\n",
    "        step = len(self.action_history)\n",
    "        print(f\"Step {step}: Chose action {action}, got reward {reward:.2f}\")\n",
    "\n",
    "    def show_history(self):\n",
    "        \"\"\"Let students see all the data we've collected.\"\"\"\n",
    "        print(f\"\\nHistory after {len(self.action_history)} steps:\")\n",
    "        print(f\"Actions chosen: {self.action_history}\")\n",
    "        print(f\"Rewards received: {[round(r, 2) for r in self.reward_history]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfdc32e-511b-4448-85da-d27859a984a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a 3-armed bandit tracker\n",
      "I'll remember every action and reward, but I won't make smart decisions yet\n",
      "\n",
      "Let's manually try some actions and see what happens:\n",
      "Step 1: Chose action 0, got reward 1.20\n",
      "Step 2: Chose action 1, got reward 0.80\n",
      "Step 3: Chose action 2, got reward -0.30\n",
      "Step 4: Chose action 0, got reward 1.50\n",
      "Step 5: Chose action 1, got reward 0.90\n",
      "Step 6: Chose action 0, got reward 1.10\n",
      "\n",
      "History after 6 steps:\n",
      "Actions chosen: [0, 1, 2, 0, 1, 0]\n",
      "Rewards received: [1.2, 0.8, -0.3, 1.5, 0.9, 1.1]\n",
      "\n",
      "Thinking questions for students:\n",
      "- Which action seems to be giving the best rewards so far?\n",
      "- How confident are you in that assessment?\n",
      "- What would you choose next, and why?\n"
     ]
    }
   ],
   "source": [
    "# Let's try it out with manual action selection\n",
    "tracker = BasicBanditTracker(3)\n",
    "#0, 1, 2\n",
    "# Simulate some manual decisions (students can try different sequences)\n",
    "manual_actions = [0, 1, 2, 0, 1, 0]  # Let students pick these\n",
    "simulated_rewards = [1.2, 0.8, -0.3, 1.5, 0.9, 1.1]  # From environment\n",
    "\n",
    "print(\"\\nLet's manually try some actions and see what happens:\")\n",
    "for action, reward in zip(manual_actions, simulated_rewards):\n",
    "    tracker.record_action_and_reward(action, reward)\n",
    "\n",
    "tracker.show_history()\n",
    "\n",
    "print(\"\\nThinking questions for students:\")\n",
    "print(\"- Which action seems to be giving the best rewards so far?\")\n",
    "print(\"- How confident are you in that assessment?\")\n",
    "print(\"- What would you choose next, and why?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863dfe9f-7715-45d2-a7f8-22a150e88789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: Adding Simple Analysis\n",
      "============================================================\n",
      "Created an analytic 3-armed bandit\n",
      "Now I can analyze my experience to make better decisions\n",
      "\n",
      "Analysis after 6 steps:\n",
      "----------------------------------------\n",
      "Action 0: 3 tries, average reward = 1.267\n",
      "  Individual rewards: [1.2, 1.5, 1.1]\n",
      "Action 1: 2 tries, average reward = 0.850\n",
      "  Individual rewards: [0.8, 0.9]\n",
      "Action 2: 1 tries, average reward = -0.300\n",
      "  Individual rewards: [-0.3]\n",
      "\n",
      "Based on current data, action 0 looks best (average reward: 1.267)\n",
      "\n",
      "Discussion points for students:\n",
      "- Is our 'best' action really the best? How can we be more confident?\n",
      "- What about actions we haven't tried much?\n",
      "- How do we balance using what we know vs. learning more?\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Adding Simple Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class AnalyticBanditTracker:\n",
    "    \"\"\"\n",
    "    Now we add the ability to analyze our data.\n",
    "    Students learn to extract insights from the raw records.\n",
    "    \"\"\"\n",
    "    def __init__(self, k_arms):\n",
    "        self.k = k_arms\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "\n",
    "        print(f\"Created an analytic {k_arms}-armed bandit\")\n",
    "        print(\"Now I can analyze my experience to make better decisions\")\n",
    "\n",
    "    def record_action_and_reward(self, action, reward):\n",
    "        self.action_history.append(action)\n",
    "        self.reward_history.append(reward)\n",
    "\n",
    "    def get_action_statistics(self):\n",
    "        \"\"\"\n",
    "        Calculate statistics for each action based on our history.\n",
    "        This is where students see how we extract knowledge from data.\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "\n",
    "        for action in range(self.k):\n",
    "            # Find all the times we chose this action\n",
    "            action_indices = [i for i, a in enumerate(self.action_history) if a == action]\n",
    "\n",
    "            if len(action_indices) == 0:\n",
    "                # We haven't tried this action yet\n",
    "                stats[action] = {\n",
    "                    'count': 0,\n",
    "                    'total_reward': 0,\n",
    "                    'average_reward': None,\n",
    "                    'rewards': []\n",
    "                }\n",
    "            else:\n",
    "                # Calculate statistics from our experience\n",
    "                rewards_for_action = [self.reward_history[i] for i in action_indices]\n",
    "                stats[action] = {\n",
    "                    'count': len(rewards_for_action),\n",
    "                    'total_reward': sum(rewards_for_action),\n",
    "                    'average_reward': sum(rewards_for_action) / len(rewards_for_action),\n",
    "                    'rewards': rewards_for_action\n",
    "                }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def print_analysis(self):\n",
    "        \"\"\"Show students what we've learned about each action.\"\"\"\n",
    "        print(f\"\\nAnalysis after {len(self.action_history)} steps:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        stats = self.get_action_statistics()\n",
    "\n",
    "        for action in range(self.k):\n",
    "            s = stats[action]\n",
    "            if s['count'] == 0:\n",
    "                print(f\"Action {action}: Never tried\")\n",
    "            else:\n",
    "                print(f\"Action {action}: {s['count']} tries, \"\n",
    "                      f\"average reward = {s['average_reward']:.3f}\")\n",
    "                print(f\"  Individual rewards: {[round(r, 2) for r in s['rewards']]}\")\n",
    "\n",
    "    def suggest_best_action(self):\n",
    "        \"\"\"\n",
    "        Simple greedy selection based on current averages.\n",
    "        Students see how to turn analysis into decisions.\n",
    "        \"\"\"\n",
    "        stats = self.get_action_statistics()\n",
    "\n",
    "        # Find the action with highest average reward (among tried actions)\n",
    "        best_action = None\n",
    "        best_average = float('-inf')\n",
    "\n",
    "        for action in range(self.k):\n",
    "            if stats[action]['count'] > 0:  # Only consider tried actions\n",
    "                if stats[action]['average_reward'] > best_average:\n",
    "                    best_average = stats[action]['average_reward']\n",
    "                    best_action = action\n",
    "\n",
    "        if best_action is not None:\n",
    "            print(f\"\\nBased on current data, action {best_action} looks best \"\n",
    "                  f\"(average reward: {best_average:.3f})\")\n",
    "        else:\n",
    "            print(\"\\nNo actions tried yet - need to explore!\")\n",
    "\n",
    "        return best_action\n",
    "\n",
    "# Demonstrate the analysis\n",
    "analyzer = AnalyticBanditTracker(3)\n",
    "\n",
    "# Use the same data as before\n",
    "for action, reward in zip(manual_actions, simulated_rewards):\n",
    "    analyzer.record_action_and_reward(action, reward)\n",
    "\n",
    "analyzer.print_analysis()\n",
    "analyzer.suggest_best_action()\n",
    "\n",
    "print(\"\\nDiscussion points for students:\")\n",
    "print(\"- Is our 'best' action really the best? How can we be more confident?\")\n",
    "print(\"- What about actions we haven't tried much?\")\n",
    "print(\"- How do we balance using what we know vs. learning more?\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50773fe6-b5ff-4740-be52-786cd91af136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Efficient Value Tracking\n",
      "============================================================\n",
      "Created an efficient 3-armed bandit\n",
      "Now I track just the essential information: averages and counts\n",
      "Let's see how the incremental averaging works:\n",
      "\n",
      "Updating estimates for action 0 with reward 1.20\n",
      "  First time trying action 0\n",
      "  New average: 1.200\n",
      "\n",
      "Current estimates:\n",
      "Action 0: 1.200 (based on 1 tries)\n",
      "Action 1: No data yet\n",
      "Action 2: No data yet\n",
      "\n",
      "Updating estimates for action 1 with reward 0.80\n",
      "  First time trying action 1\n",
      "  New average: 0.800\n",
      "\n",
      "Current estimates:\n",
      "Action 0: 1.200 (based on 1 tries)\n",
      "Action 1: 0.800 (based on 1 tries)\n",
      "Action 2: No data yet\n",
      "\n",
      "Updating estimates for action 2 with reward -0.30\n",
      "  First time trying action 2\n",
      "  New average: -0.300\n",
      "\n",
      "Current estimates:\n",
      "Action 0: 1.200 (based on 1 tries)\n",
      "Action 1: 0.800 (based on 1 tries)\n",
      "Action 2: -0.300 (based on 1 tries)\n",
      "\n",
      "Updating estimates for action 0 with reward 1.50\n",
      "  Old average: 1.200 (based on 1.0 tries)\n",
      "  New average: 1.350 (based on 2.0 tries)\n",
      "  Change: 0.150\n",
      "\n",
      "Current estimates:\n",
      "Action 0: 1.350 (based on 2 tries)\n",
      "Action 1: 0.800 (based on 1 tries)\n",
      "Action 2: -0.300 (based on 1 tries)\n",
      "\n",
      "Updating estimates for action 1 with reward 0.90\n",
      "  Old average: 0.800 (based on 1.0 tries)\n",
      "  New average: 0.850 (based on 2.0 tries)\n",
      "  Change: 0.050\n",
      "\n",
      "Current estimates:\n",
      "Action 0: 1.350 (based on 2 tries)\n",
      "Action 1: 0.850 (based on 2 tries)\n",
      "Action 2: -0.300 (based on 1 tries)\n",
      "\n",
      "Updating estimates for action 0 with reward 1.10\n",
      "  Old average: 1.350 (based on 2.0 tries)\n",
      "  New average: 1.267 (based on 3.0 tries)\n",
      "  Change: -0.083\n",
      "\n",
      "Current estimates:\n",
      "Action 0: 1.267 (based on 3 tries)\n",
      "Action 1: 0.850 (based on 2 tries)\n",
      "Action 2: -0.300 (based on 1 tries)\n",
      "\n",
      "Greedy choice: Action 0\n",
      "\n",
      "Key insight for students:\n",
      "Notice that we get the same averages as before, but we don't need to store all the data!\n",
      "The incremental formula gives us mathematical efficiency.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 3: Efficient Value Tracking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class EfficientBanditTracker:\n",
    "    \"\"\"\n",
    "    Now we introduce the efficient incremental averaging approach.\n",
    "    Students see why we don't need to store all history.\n",
    "    \"\"\"\n",
    "    def __init__(self, k_arms):\n",
    "        self.k = k_arms\n",
    "\n",
    "        # Instead of storing all history, we just track what we need\n",
    "        self.Q = np.zeros(k_arms)      # Average rewards (our estimates)\n",
    "        self.N = np.zeros(k_arms)      # How many times we've tried each action\n",
    "\n",
    "        print(f\"Created an efficient {k_arms}-armed bandit\")\n",
    "        print(\"Now I track just the essential information: averages and counts\")\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update our estimates using the incremental average formula.\n",
    "        This is where students learn the key algorithmic insight.\n",
    "        \"\"\"\n",
    "        print(f\"\\nUpdating estimates for action {action} with reward {reward:.2f}\")\n",
    "\n",
    "        # Show the old values\n",
    "        old_average = self.Q[action]\n",
    "        old_count = self.N[action]\n",
    "\n",
    "        # Update count\n",
    "        self.N[action] += 1\n",
    "        new_count = self.N[action]\n",
    "\n",
    "        # Calculate new average using incremental formula\n",
    "        # Q_new = Q_old + (1/N) * (reward - Q_old)\n",
    "        self.Q[action] = old_average + (reward - old_average) / new_count\n",
    "        new_average = self.Q[action]\n",
    "\n",
    "        # Show students what happened\n",
    "        if old_count == 0:\n",
    "            print(f\"  First time trying action {action}\")\n",
    "            print(f\"  New average: {new_average:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Old average: {old_average:.3f} (based on {old_count} tries)\")\n",
    "            print(f\"  New average: {new_average:.3f} (based on {new_count} tries)\")\n",
    "            print(f\"  Change: {new_average - old_average:.3f}\")\n",
    "\n",
    "    def show_current_estimates(self):\n",
    "        \"\"\"Display our current knowledge about each action.\"\"\"\n",
    "        print(f\"\\nCurrent estimates:\")\n",
    "        for action in range(self.k):\n",
    "            if self.N[action] == 0:\n",
    "                print(f\"Action {action}: No data yet\")\n",
    "            else:\n",
    "                print(f\"Action {action}: {self.Q[action]:.3f} \"\n",
    "                      f\"(based on {int(self.N[action])} tries)\")\n",
    "\n",
    "    def get_best_action(self):\n",
    "        \"\"\"Simple greedy action selection.\"\"\"\n",
    "        # Only consider actions we've tried\n",
    "        tried_actions = [a for a in range(self.k) if self.N[a] > 0]\n",
    "\n",
    "        if len(tried_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        best_action = max(tried_actions, key=lambda a: self.Q[a])\n",
    "        return best_action\n",
    "\n",
    "# Demonstrate efficient tracking\n",
    "efficient = EfficientBanditTracker(3)\n",
    "\n",
    "print(\"Let's see how the incremental averaging works:\")\n",
    "for action, reward in zip(manual_actions, simulated_rewards):\n",
    "    efficient.update_estimates(action, reward)\n",
    "    efficient.show_current_estimates()\n",
    "\n",
    "best = efficient.get_best_action()\n",
    "if best is not None:\n",
    "    print(f\"\\nGreedy choice: Action {best}\")\n",
    "\n",
    "print(\"\\nKey insight for students:\")\n",
    "print(\"Notice that we get the same averages as before, but we don't need to store all the data!\")\n",
    "print(\"The incremental formula gives us mathematical efficiency.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49faf331-1edd-4c13-9195-03e49a95dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: Adding Exploration Strategy\n",
      "============================================================\n",
      "Complete learning demonstration:\n",
      "----------------------------------------\n",
      "Environment created with true action values: [0.5, 1.2, 0.8]\n",
      "(The bandit doesn't know these - it has to learn them!)\n",
      "Created epsilon-greedy bandit with ε = 0.2\n",
      "Now I can balance exploration and exploitation!\n",
      "Step 1: Trying untested action 0\n",
      "  → Received reward: 0.73\n",
      "Step 2: Trying untested action 1\n",
      "  → Received reward: 0.77\n",
      "Step 3: Trying untested action 2\n",
      "  → Received reward: 1.06\n",
      "\n",
      "Current state of learning:\n",
      "Action 0: value = 0.728, tried 1 times (low confidence)\n",
      "Action 1: value = 0.773, tried 1 times (low confidence)\n",
      "Action 2: value = 1.056, tried 1 times (low confidence)\n",
      "\n",
      "Step 4: Exploiting - chose best action 2 (value: 1.056)\n",
      "  → Received reward: 0.36\n",
      "Step 5: Exploring - chose random action 2\n",
      "  → Received reward: 0.29\n",
      "Step 6: Exploiting - chose best action 1 (value: 0.773)\n",
      "  → Received reward: 1.26\n",
      "\n",
      "Current state of learning:\n",
      "Action 0: value = 0.728, tried 1 times (low confidence)\n",
      "Action 1: value = 1.017, tried 2 times (low confidence)\n",
      "Action 2: value = 0.570, tried 3 times (low confidence)\n",
      "\n",
      "Step 7: Exploiting - chose best action 1 (value: 1.017)\n",
      "  → Received reward: 0.92\n",
      "Step 8: Exploiting - chose best action 1 (value: 0.986)\n",
      "  → Received reward: 1.04\n",
      "Step 9: Exploiting - chose best action 1 (value: 1.000)\n",
      "  → Received reward: 1.44\n",
      "\n",
      "Current state of learning:\n",
      "Action 0: value = 0.728, tried 1 times (low confidence)\n",
      "Action 1: value = 1.089, tried 5 times (high confidence)\n",
      "Action 2: value = 0.570, tried 3 times (low confidence)\n",
      "\n",
      "Step 10: Exploiting - chose best action 1 (value: 1.089)\n",
      "  → Received reward: 0.90\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 4: Adding Exploration Strategy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SimpleEpsilonGreedy:\n",
    "    \"\"\"\n",
    "    Finally, we add exploration to create a complete learning algorithm.\n",
    "    Students see how all the pieces come together.\n",
    "    \"\"\"\n",
    "    def __init__(self, k_arms, epsilon=0.1):\n",
    "        self.k = k_arms\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # All the efficient tracking from before\n",
    "        self.Q = np.zeros(k_arms)\n",
    "        self.N = np.zeros(k_arms)\n",
    "\n",
    "        # Track total steps for analysis\n",
    "        self.total_steps = 0\n",
    "\n",
    "        print(f\"Created epsilon-greedy bandit with ε = {epsilon}\")\n",
    "        print(\"Now I can balance exploration and exploitation!\")\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"\n",
    "        The epsilon-greedy strategy students have been building toward.\n",
    "        \"\"\"\n",
    "        self.total_steps += 1\n",
    "\n",
    "        # Handle the case where we haven't tried any actions yet\n",
    "        untried_actions = [a for a in range(self.k) if self.N[a] == 0]\n",
    "        if len(untried_actions) > 0:\n",
    "            action = np.random.choice(untried_actions)\n",
    "            print(f\"Step {self.total_steps}: Trying untested action {action}\")\n",
    "            return action\n",
    "\n",
    "        # Epsilon-greedy decision\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose randomly\n",
    "            action = np.random.randint(self.k)\n",
    "            print(f\"Step {self.total_steps}: Exploring - chose random action {action}\")\n",
    "        else:\n",
    "            # Exploit: choose the best known action\n",
    "            action = np.argmax(self.Q)\n",
    "            print(f\"Step {self.total_steps}: Exploiting - chose best action {action} \"\n",
    "                  f\"(value: {self.Q[action]:.3f})\")\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn_from_reward(self, action, reward):\n",
    "        \"\"\"Update our knowledge (same as before).\"\"\"\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
    "\n",
    "    def show_learning_progress(self):\n",
    "        \"\"\"Show students how the algorithm is learning.\"\"\"\n",
    "        print(\"\\nCurrent state of learning:\")\n",
    "        for action in range(self.k):\n",
    "            if self.N[action] == 0:\n",
    "                print(f\"Action {action}: Never tried\")\n",
    "            else:\n",
    "                confidence = \"high\" if self.N[action] >= 5 else \"low\"\n",
    "                print(f\"Action {action}: value = {self.Q[action]:.3f}, \"\n",
    "                      f\"tried {int(self.N[action])} times ({confidence} confidence)\")\n",
    "\n",
    "# Create a simple environment for demonstration\n",
    "class SimpleEnvironment:\n",
    "    \"\"\"A basic environment that students can understand easily.\"\"\"\n",
    "    def __init__(self):\n",
    "        # True values that the bandit needs to learn\n",
    "        self.true_values = [0.5, 1.2, 0.8]  # Action 1 is actually best\n",
    "        print(f\"Environment created with true action values: {self.true_values}\")\n",
    "        print(\"(The bandit doesn't know these - it has to learn them!)\")\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        \"\"\"Return a noisy reward based on the true value.\"\"\"\n",
    "        true_value = self.true_values[action]\n",
    "        # Add some randomness so the bandit can't learn immediately\n",
    "        noise = np.random.normal(0, 0.3)\n",
    "        return true_value + noise\n",
    "\n",
    "# Final demonstration: complete learning loop\n",
    "print(\"Complete learning demonstration:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "env = SimpleEnvironment()\n",
    "agent = SimpleEpsilonGreedy(3, epsilon=0.2)\n",
    "\n",
    "# Run several steps so students can see the learning process\n",
    "for step in range(10):\n",
    "    action = agent.choose_action()\n",
    "    reward = env.get_reward(action)\n",
    "    agent.learn_from_reward(action, reward)\n",
    "\n",
    "    print(f\"  → Received reward: {reward:.2f}\")\n",
    "\n",
    "    if step % 3 == 2:  # Show progress every few steps\n",
    "        agent.show_learning_progress()\n",
    "        print()\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10deca70-0456-4c46-b576-bfd303307c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
